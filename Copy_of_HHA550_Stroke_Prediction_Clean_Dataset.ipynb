{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c804435d",
      "metadata": {
        "id": "c804435d"
      },
      "source": [
        "# HHA550_Stroke Prediction Dataset\n",
        "\n",
        "\n",
        "\n",
        "## Healtcare-dataset-stroke-data\n",
        "\n",
        "#### (Check Modules folders for csv and ipynb for each class)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdf8f594",
      "metadata": {
        "id": "cdf8f594"
      },
      "source": [
        "# DATA\n",
        "## Stroke Prediction Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ab60656",
      "metadata": {
        "id": "8ab60656"
      },
      "source": [
        "#### \n",
        "Context\n",
        "\n",
        "According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths. This dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.\n",
        "\n",
        "Attribute Information\n",
        "\n",
        "    1) id: unique identifier\n",
        "\n",
        "    2) gender: \"Male\", \"Female\" or \"Other\"\n",
        "\n",
        "    3) age: age of the patient\n",
        "\n",
        "    4) hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n",
        "\n",
        "    5) heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n",
        "\n",
        "    6) ever_married: \"No\" or \"Yes\"\n",
        "\n",
        "    7) work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n",
        "\n",
        "    8) Residence_type: \"Rural\" or \"Urban\"\n",
        "\n",
        "    9) avg_glucose_level: average glucose level in blood\n",
        "\n",
        "    10) bmi: body mass index\n",
        "\n",
        "    11) smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n",
        "\n",
        "    12) stroke: 1 if the patient had a stroke or 0 if not\n",
        "\n",
        "*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ac477eb",
      "metadata": {
        "id": "7ac477eb"
      },
      "source": [
        "# .CSV Data\n",
        "### Healtcare-dataset-stroke-data.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3d98fc5",
      "metadata": {
        "id": "d3d98fc5"
      },
      "source": [
        "# IMPORTING Everthing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a10f0b17",
      "metadata": {
        "id": "a10f0b17"
      },
      "outputs": [],
      "source": [
        "# Commands to install some of the libraries in-case if they are not installed\n",
        "# Any other library that needs to be installed just use: !pip install <library name>\n",
        "# !pip install seaborn\n",
        "# !pip install missingno\n",
        "# !pip install xgboost\n",
        "# !pip install catboost\n",
        "# !pip install regex\n",
        "# !pip install sklearn\n",
        "# !pip install pandas\n",
        "# !pip install numpy\n",
        "# !pip install imblearn\n",
        "# !pip install lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4e845f77",
      "metadata": {
        "id": "4e845f77"
      },
      "outputs": [],
      "source": [
        "import pandas as pd   # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import numpy as np   # linear algebra\n",
        "import matplotlib.pyplot as plt  #graphs and plots\n",
        "import seaborn as sns   #data visualizations \n",
        "import csv # Some extra functionalities for csv  files - reading it as a dictionary\n",
        "from lightgbm import LGBMClassifier #sklearn is for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction \n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_validate   #break up dataset into train and test sets\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "\n",
        "# importing python library for working with missing data\n",
        "import missingno as msno\n",
        "# To install missingno use: !pip install missingno\n",
        "import re    # This library is used to perform regex pattern matching\n",
        "\n",
        "# import various functions from sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, classification_report, make_scorer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "910c0044",
      "metadata": {
        "id": "910c0044"
      },
      "source": [
        "Import additional items as needed...\n",
        "We may not use them all in this course..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ee3d031a",
      "metadata": {
        "id": "ee3d031a"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold,cross_val_score, RepeatedStratifiedKFold,StratifiedKFold\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.preprocessing import OneHotEncoder,StandardScaler,PowerTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import KNNImputer,SimpleImputer\n",
        "from sklearn.compose import make_column_transformer\n",
        "from imblearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score,\\\n",
        "                            precision_score, recall_score, roc_auc_score,\\\n",
        "                            plot_confusion_matrix, classification_report, plot_roc_curve, f1_score\n",
        "\n",
        "import plotly \n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "import plotly.offline as py\n",
        "from plotly.offline import iplot\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9136179d",
      "metadata": {
        "id": "9136179d"
      },
      "source": [
        "* If from imblearn.oversampling import SMOTE does not load use\n",
        "    `conda install -c conda-forge imbalanced-learn`\n",
        "* Then rerun\n",
        "    `from imblearn.over_sampling import SMOTE`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb1b5bfa",
      "metadata": {
        "id": "eb1b5bfa"
      },
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fe20041",
      "metadata": {
        "id": "2fe20041"
      },
      "source": [
        "## Start with Loading the CSV Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "def56f62",
      "metadata": {
        "id": "def56f62"
      },
      "outputs": [],
      "source": [
        "#upload the csv and declare its name to = the csv\n",
        "#for this course we will name the dataframe 'stroke', but you can change it to df or anything else you want.\n",
        "\n",
        "#Find the file path to the .csv\n",
        "    #Then use code below to read .csv\n",
        "# stroke = pd.read_csv('x')\n",
        "\n",
        "#for PC file path will use /  \n",
        "         #Original PC file path 'C:\\Users\\kaden\\Desktop\\HHA550_Stroke_Data\\healthcare-dataset-stroke-data.csv'\n",
        "         #Change PC file path for Python to C:/Users/kaden/Desktop/Python_Diabetes Dataset/dataset_diabetes/diabetic_data.csv'\n",
        "    #for MAC file path will use //\n",
        "         #Original MAC file path 'C://Users//kaden//Desktop//HHA550_Stroke_Data//healthcare-dataset-stroke-data.csv'\n",
        "         #Change MAC file path for Python to 'C://Users//kaden//Desktop//HHA550_Stroke_Data//healthcare-dataset-stroke-data.csv'\n",
        "\n",
        "#for PC (running PC for this example)\n",
        "# stroke = pd.read_csv('C:/Users/kaden/Desktop/HHA550_Stroke_Data/healthcare-dataset-stroke-data.csv')\n",
        "stroke = pd.read_csv('https://raw.githubusercontent.com/michaelalam/HHA-550-Stroke-Data/main/healthcare-dataset-stroke-data.csv')\n",
        "\n",
        "#for MAC\n",
        "#stroke = pd.read_csv('C://Users//kaden//Desktop//HHA550_Stroke_Data//healthcare-dataset-stroke-data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d3afce7",
      "metadata": {
        "id": "0d3afce7"
      },
      "source": [
        "# Breaking the data up into Train & Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "23150ab7",
      "metadata": {
        "id": "23150ab7"
      },
      "outputs": [],
      "source": [
        "train_df, valid_df, test_df = np.split(stroke.sample(frac=1, random_state=42), \n",
        "                                       [int(.7*len(stroke)), int(0.85*len(stroke))])\n",
        "train_df = train_df.reset_index(drop = True)\n",
        "valid_df = valid_df.reset_index(drop = True)\n",
        "test_df = test_df.reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ae84eff3",
      "metadata": {
        "id": "ae84eff3",
        "outputId": "3c1a1999-3e01-4140-bd01-00d56994e4d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4861\n",
              "1     249\n",
              "Name: stroke, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "stroke.stroke.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "689c1912",
      "metadata": {
        "id": "689c1912",
        "outputId": "84d561b0-d53a-4c14-a45e-8cf128fb5e00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    3404\n",
              "1     173\n",
              "Name: stroke, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "train_df.stroke.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "546fc97f",
      "metadata": {
        "id": "546fc97f",
        "outputId": "b7e55c92-2360-40b8-debd-95ac6a4c5dc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    726\n",
              "1     40\n",
              "Name: stroke, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "valid_df.stroke.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "6d1b8408",
      "metadata": {
        "id": "6d1b8408",
        "outputId": "2132f16a-1ff7-47a2-85c5-618b114063c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    731\n",
              "1     36\n",
              "Name: stroke, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "test_df.stroke.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cad6054",
      "metadata": {
        "id": "0cad6054"
      },
      "source": [
        "# Treating the Imbalance in the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b80ce827",
      "metadata": {
        "id": "b80ce827"
      },
      "source": [
        "Imbalance in the data means that one of the classes in the data is too less as compared to the others. Typically, it is better to balance the data in some way to give the positives more weight. There are 3 strategies that are typically utilized:\n",
        "\n",
        "* Sub-sample the more dominant class: use a random subset of the negatives\n",
        "* Over-sample the imbalanced class: use the same positive samples multiple times\n",
        "* Create synthetic positive data\n",
        "\n",
        "Usually, you will want to use the latter two methods if you only have a handful of positive cases. Since we have a few thousand positive cases, let's use the sub-sample approach. Here, we will create a balanced training data set that has 50% positive and 50% negative. You can also play with this ratio to see if you can get an improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7d35fb6",
      "metadata": {
        "id": "c7d35fb6"
      },
      "outputs": [],
      "source": [
        "def calc_prevalence(y_actual):\n",
        "    \n",
        "    '''\n",
        "    This function is to understand the ratio/distribution of the classes that we are going to predict for.\n",
        "    \n",
        "    Params:\n",
        "    1. y_actual: The target feature\n",
        "    \n",
        "    Return:\n",
        "    1. (sum(y_actual)/len(y_actual)): The ratio of the postive class in the comlpete data.\n",
        "    '''\n",
        "    \n",
        "    return (sum(y_actual)/len(y_actual))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a9dfc36",
      "metadata": {
        "id": "6a9dfc36"
      },
      "outputs": [],
      "source": [
        "# split the training data into positive and negative\n",
        "rows_pos = train_df.stroke == 1\n",
        "df_train_pos = train_df.loc[rows_pos]\n",
        "df_train_neg = train_df.loc[~rows_pos]\n",
        "\n",
        "# merge the balanced data\n",
        "stroke_df_balanced = pd.concat([df_train_pos, df_train_neg.sample(n = len(df_train_pos), random_state = 111)],axis = 0)\n",
        "\n",
        "# shuffle the order of training samples \n",
        "stroke_df_balanced = stroke_df_balanced.sample(n = len(stroke_df_balanced), random_state = 42).reset_index(drop = True)\n",
        "\n",
        "print('Train balanced prevalence(n = %d):%.3f'%(len(stroke_df_balanced), \\\n",
        "                                                calc_prevalence(stroke_df_balanced.stroke.values)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "938bbaef",
      "metadata": {
        "id": "938bbaef"
      },
      "outputs": [],
      "source": [
        "stroke_df_balanced.stroke.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7590ce92",
      "metadata": {
        "id": "7590ce92"
      },
      "outputs": [],
      "source": [
        "X_train = stroke_df_balanced.drop('stroke',axis=1)\n",
        "\n",
        "y_train = stroke_df_balanced['stroke']\n",
        "\n",
        "X_valid = valid_df.drop('stroke',axis=1)\n",
        "\n",
        "y_valid = valid_df['stroke']\n",
        "\n",
        "X_test = test_df.drop('stroke',axis=1)\n",
        "\n",
        "y_test = test_df['stroke']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37b5c305",
      "metadata": {
        "id": "37b5c305"
      },
      "outputs": [],
      "source": [
        "scaler=StandardScaler()\n",
        "X_train[['age', 'bmi', 'avg_glucose_level']] = pd.DataFrame(scaler.fit_transform(X_train[['age', 'bmi', 'avg_glucose_level']]),columns=['age', 'bmi', 'avg_glucose_level'])\n",
        "X_valid[['age', 'bmi', 'avg_glucose_level']] = pd.DataFrame(scaler.transform(X_valid[['age', 'bmi', 'avg_glucose_level']]),columns=['age', 'bmi', 'avg_glucose_level'])\n",
        "X_test[['age', 'bmi', 'avg_glucose_level']] = pd.DataFrame(scaler.transform(X_test[['age', 'bmi', 'avg_glucose_level']]),columns=['age', 'bmi', 'avg_glucose_level'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b1e5723",
      "metadata": {
        "id": "9b1e5723"
      },
      "source": [
        "# Creating and Understanding Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecf8b577",
      "metadata": {
        "id": "ecf8b577"
      },
      "outputs": [],
      "source": [
        "def calc_specificity(y_actual, y_pred, thresh):\n",
        "    # calculates specificity\n",
        "    return sum((y_pred < thresh) & (y_actual == 0)) /sum(y_actual ==0)\n",
        "\n",
        "def print_report(y_actual, y_pred, thresh = 0.5):\n",
        "    \n",
        "    '''\n",
        "    This function calculates all the metrics to asses the machine learning models.\n",
        "    \n",
        "    Params:\n",
        "    1. y_actual: The actual values for the target variable.\n",
        "    2. y_pred: The predicted values for the target variable.\n",
        "    3. thresh: The threshold for the probability to be considered as a positive class. Default value 0.5\n",
        "    \n",
        "    Return:\n",
        "    1. AUC\n",
        "    2. Accuracy\n",
        "    3. Recall\n",
        "    4. Precision\n",
        "    5. Specificity\n",
        "    '''\n",
        "    \n",
        "    auc = roc_auc_score(y_actual, y_pred)\n",
        "    accuracy = accuracy_score(y_actual, (y_pred > thresh))\n",
        "    recall = recall_score(y_actual, (y_pred > thresh))\n",
        "    precision = precision_score(y_actual, (y_pred > thresh))\n",
        "    specificity = calc_specificity(y_actual, y_pred, thresh)\n",
        "    print('AUC:%.3f'%auc)\n",
        "    print('accuracy:%.3f'%accuracy)\n",
        "    print('recall:%.3f'%recall)\n",
        "    print('precision:%.3f'%precision)\n",
        "    print('specificity:%.3f'%specificity)\n",
        "    print('prevalence:%.3f'%calc_prevalence(y_actual))\n",
        "    print(' ')\n",
        "    return auc, accuracy, recall, precision, specificity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ba8c8ea",
      "metadata": {
        "id": "3ba8c8ea"
      },
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d02dbec4",
      "metadata": {
        "id": "d02dbec4"
      },
      "outputs": [],
      "source": [
        "lnr = LinearRegression()\n",
        "lnr.fit(X_train, y_train)\n",
        "\n",
        "y_valid_preds = lnr.predict(X_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef7f15a8",
      "metadata": {
        "id": "ef7f15a8"
      },
      "outputs": [],
      "source": [
        "y_valid_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "457c7731",
      "metadata": {
        "id": "457c7731"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5086c49a",
      "metadata": {
        "id": "5086c49a"
      },
      "outputs": [],
      "source": [
        "lr=LogisticRegression(random_state = 42, solver = 'newton-cg', max_iter = 200)\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_valid_preds = lr.predict_proba(X_valid)[:,1]\n",
        "\n",
        "print('Metrics for Validation data:')\n",
        "\n",
        "lr_valid_auc, lr_valid_accuracy, lr_valid_recall, \\\n",
        "    lr_valid_precision, lr_valid_specificity = print_report(y_valid,y_valid_preds, 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdba01c6",
      "metadata": {
        "id": "bdba01c6"
      },
      "source": [
        "## Explaining Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7f93713",
      "metadata": {
        "id": "a7f93713"
      },
      "outputs": [],
      "source": [
        "from IPython import display\n",
        "display.Image(\"C:/Users/kaden/Desktop/HHA550_Stroke_Data/Explaining_Results.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dbc40ab",
      "metadata": {
        "id": "1dbc40ab"
      },
      "source": [
        "#### AUC (area under the curve)\n",
        "* values 0 to 1\n",
        "* higher the AUC (closer to \"1\"), the better the performace of the model at distinguishing between the positive and negative clases.\n",
        "\n",
        "#### Precision\n",
        "* ratio between the correct predecitions and the total predicitons\n",
        "* indicates how good is the model at watever it predicts\n",
        "\n",
        "#### Recall\n",
        "* (Sensitivity)\n",
        "* Precision = TP/TP+F \n",
        "* Precision is the ratio of correctly predicted positive observations to the total predicted positive observations\n",
        "\n",
        "#### f1-score \n",
        "* F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n",
        "* is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy\n",
        "\n",
        "#### Support\n",
        "* positive counts (values)\n",
        "\n",
        "#### Accuracy\n",
        "* how close a measurement is to the true value\n",
        "* Accuracy = TP+TN/TP+FP+FN+TN\n",
        "* is simply a ratio of correctly predicted observation to the total observations.\n",
        "\n",
        "#### Macro avg\n",
        "* all classes equally contribute to the final averaged metric\n",
        "\n",
        "#### Weighted avg\n",
        "* each classesâ€™s contribution to the average is weighted by its size\n",
        "\n",
        "#### Important Insights into Results\n",
        "* If on average, your measurements for a given substance are close to the known value, but the measurements are far from each other, then you have accuracy without precision\n",
        "* Which is better accuracy or precision?\n",
        "** Accuracy is something you can fix in future measurements. Precision is more important in calculations. When using a measured value in a calculation, you can only be as precise as your least precise measurement\n",
        "* Is precision more important than recall?\n",
        "** Recall is more important where Overlooked Cases (False Negatives) are more costly than False Alarms (False Positive). Precision is more important where False Alarms (False Positives) are more costly than Overlooked Cases (False Negatives). The focus in these problems is in weeding out the negative cases.\n",
        "* F1 Score might be a better measure to use if we need to seek a balance between Precision and Recall AND there is an uneven class distribution (large number of Actual Negatives).\n",
        "\n",
        "\n",
        "#### Helpful Links\n",
        "* https://towardsdatascience.com/choosing-performance-metrics-61b40819eae1#:~:text=The%20macro%20average%20precision%20is,less%20in%20the%20weighted%20average. \n",
        "* https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9 \n",
        "* https://www.mariakhalusova.com/posts/2019-04-17-ml-model-evaluation-metrics-p2/#:~:text=Micro%2Daveraged%3A%20all%20samples%20equally,is%20weighted%20by%20its%20size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db547a52",
      "metadata": {
        "id": "db547a52"
      },
      "source": [
        "### Summary of Results (what is the most important?)\n",
        "#### Logistic Regression vs Linear Regression results\n",
        "* As per our use case we need the results to be classes i.e. Stroke or No-Stroke for which we need a model that works on classification.\n",
        "* Using the Linear Regression as seen above is not giving us the results as 0 or 1 instead it is giving us the results in numerical form. The internal logic of linear regression does not work output the probability it just tries to get the output as close to the target in the training data. Whereas the logistic regression model works on probability principle.\n",
        "* Linear regression would try to fit the data if it can be fit using straight line. If we just use regression then it can try to fit a curved surface too. Whereas the graph of Logistic regression takes the shape of a sigmoid. See the pictures below."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "548b0fc4",
      "metadata": {
        "id": "548b0fc4"
      },
      "source": [
        "<p>\n",
        "<img src = \"LinearRegression.PNG\">\n",
        "</p>\n",
        "\n",
        "<b><center>Analytics Vidhya - Fig.1 - Linear Regression Example</center></b>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3368963d",
      "metadata": {
        "id": "3368963d"
      },
      "source": [
        "<p>\n",
        "<img src = \"LogisticRegression.PNG\">\n",
        "</p>\n",
        "\n",
        "<b><center>Analytics Vidhya - Fig.2 - Logistic Regression Example</center></b>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4691cdbb",
      "metadata": {
        "id": "4691cdbb"
      },
      "source": [
        "Lets look at some other models to see if we get better results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3555e949",
      "metadata": {
        "id": "3555e949"
      },
      "source": [
        "## KNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cba627ad",
      "metadata": {
        "id": "cba627ad"
      },
      "outputs": [],
      "source": [
        "knn = KNeighborsClassifier(n_neighbors = 100)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "knn_preds = knn.predict_proba(X_valid)[:,1]\n",
        "\n",
        "lr_valid_auc, lr_valid_accuracy, lr_valid_recall, \\\n",
        "    lr_valid_precision, lr_valid_specificity = print_report(y_valid,knn_preds, 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2268172",
      "metadata": {
        "id": "f2268172"
      },
      "source": [
        "## Stochastic Gradient Descent Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "167f592a",
      "metadata": {
        "id": "167f592a"
      },
      "outputs": [],
      "source": [
        "sgdc=SGDClassifier(loss = 'log',alpha = 0.1,random_state = 42)\n",
        "sgdc.fit(X_train, y_train)\n",
        "\n",
        "sgd_preds = sgdc.predict_proba(X_valid)[:,1]\n",
        "\n",
        "print('Stochastic Gradient Descent')\n",
        "print('Validation:')\n",
        "sgdc_valid_auc, sgdc_valid_accuracy, sgdc_valid_recall, \\\n",
        "                sgdc_valid_precision, sgdc_valid_specificity = print_report(y_valid,sgd_preds, 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e841c95",
      "metadata": {
        "id": "4e841c95"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c524b7f",
      "metadata": {
        "id": "5c524b7f"
      },
      "outputs": [],
      "source": [
        "dc_clf = DecisionTreeClassifier(random_state=42, max_depth = 10)\n",
        "dc_clf.fit(X_train, y_train)\n",
        "\n",
        "dc_preds_proba = dc_clf.predict_proba(X_valid)[:,1]\n",
        "dc_preds = dc_clf.predict(X_valid)\n",
        "\n",
        "lr_valid_auc, lr_valid_accuracy, lr_valid_recall, \\\n",
        "    lr_valid_precision, lr_valid_specificity = print_report(y_valid,dc_preds_proba, 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a24a7e5b",
      "metadata": {
        "id": "a24a7e5b"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "009ae20e",
      "metadata": {
        "id": "009ae20e"
      },
      "outputs": [],
      "source": [
        "rf_clf = RandomForestClassifier(random_state=111, max_depth = 6)\n",
        "\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "rf_preds = rf_clf.predict(X_valid)\n",
        "rf_preds_proba = rf_clf.predict_proba(X_valid)[:, 1]\n",
        "\n",
        "lr_valid_auc, lr_valid_accuracy, lr_valid_recall, \\\n",
        "    lr_valid_precision, lr_valid_specificity = print_report(y_valid,rf_preds_proba, 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "984ea500",
      "metadata": {
        "id": "984ea500"
      },
      "source": [
        "## Linear SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "893e28ff",
      "metadata": {
        "id": "893e28ff"
      },
      "outputs": [],
      "source": [
        "lsvc_clf = LinearSVC(random_state=111)\n",
        "lsvc_clf.fit(X_train, y_train)\n",
        "\n",
        "lsvc_preds = lsvc_clf.decision_function(X_valid)\n",
        "\n",
        "lr_valid_auc, lr_valid_accuracy, lr_valid_recall, \\\n",
        "    lr_valid_precision, lr_valid_specificity = print_report(y_valid,lsvc_preds, 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d2d1087",
      "metadata": {
        "id": "1d2d1087"
      },
      "source": [
        "## Gradient Boosting Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6e1e3d5",
      "metadata": {
        "id": "c6e1e3d5"
      },
      "outputs": [],
      "source": [
        "gb_clf = GradientBoostingClassifier(n_estimators = 100, criterion='friedman_mse', learning_rate = 1.0, max_depth = 3,\\\n",
        "                                    random_state = 111)\n",
        "\n",
        "gb_clf.fit(X_train, y_train)\n",
        "gb_preds = gb_clf.predict(X_valid)\n",
        "gb_preds_proba = gb_clf.predict_proba(X_valid)[:, 1]\n",
        "\n",
        "lr_valid_auc, lr_valid_accuracy, lr_valid_recall, \\\n",
        "    lr_valid_precision, lr_valid_specificity = print_report(y_valid,gb_preds_proba, 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdfe0a53",
      "metadata": {
        "id": "bdfe0a53"
      },
      "source": [
        "## XGB Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83f18546",
      "metadata": {
        "id": "83f18546"
      },
      "outputs": [],
      "source": [
        "xgb_clf = xgb.XGBClassifier(max_depth=3, learning_rate = 1.0, use_label_encoder = False,\\\n",
        "                            eval_metric = 'logloss')\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "xgb_preds = xgb_clf.predict(X_valid)\n",
        "xgb_preds_proba = xgb_clf.predict_proba(X_valid)[:, 1]\n",
        "\n",
        "lr_valid_auc, lr_valid_accuracy, lr_valid_recall, \\\n",
        "    lr_valid_precision, lr_valid_specificity = print_report(y_valid,xgb_preds_proba, 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70bc6c12",
      "metadata": {
        "id": "70bc6c12"
      },
      "source": [
        "## Catboost Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbba6d6d",
      "metadata": {
        "scrolled": true,
        "id": "bbba6d6d"
      },
      "outputs": [],
      "source": [
        "catb=CatBoostClassifier(iterations=200, depth=3, learning_rate=1.0, random_state = 111)\n",
        "catb.fit(X_train, y_train)\n",
        "catb_preds = catb.predict_proba(X_valid)[:, 1]\n",
        "\n",
        "lr_valid_auc, lr_valid_accuracy, lr_valid_recall, \\\n",
        "    lr_valid_precision, lr_valid_specificity = print_report(y_valid,catb_preds, 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6128b0e",
      "metadata": {
        "id": "e6128b0e"
      },
      "source": [
        "# Hyper Parameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ccb5ce3",
      "metadata": {
        "id": "6ccb5ce3"
      },
      "source": [
        "* From the above models we will choose two models for demonstration i.e. Random Forest, Decision Trees for hyper-parameter tuning.\n",
        "* Generally you can pick up the top three models based on the 'AUC', 'Recall' or 'F1 score' score and tune them.\n",
        "\n",
        "There are many techniques for hyper-parameter tuning:\n",
        "\n",
        "* Random Search\n",
        "* Grid Search\n",
        "* Halving Grid Search(added recently in sklearn)\n",
        "\n",
        "Special Note:\n",
        "* It will take significant time to run Hyper Parameter Tuning \n",
        "* Timing will depend on available resources of server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "367dc1c8",
      "metadata": {
        "id": "367dc1c8"
      },
      "outputs": [],
      "source": [
        "recall_scoring = make_scorer(recall_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f75172b",
      "metadata": {
        "id": "2f75172b"
      },
      "source": [
        "## Decision Tree - Hyper Parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4a7af70",
      "metadata": {
        "id": "c4a7af70"
      },
      "outputs": [],
      "source": [
        "dc_grid = {'max_features':['auto','sqrt'], # maximum number of features to use at each split\n",
        "           'max_depth':range(1,11,1), # maximum depth of the tree\n",
        "           'min_samples_split':range(2,10,2), # minimum number of samples to split a node\n",
        "           'criterion':['gini','entropy']} # criterion for evaluating a split\n",
        "\n",
        "dc_random = RandomizedSearchCV(estimator = dc_clf, param_distributions = dc_grid, \n",
        "                               n_iter = 20, cv = 2, scoring=recall_scoring,\n",
        "                               verbose = 1, random_state = 111)\n",
        "\n",
        "dc_random.fit(X_train, y_train)\n",
        "\n",
        "dc_random.best_params_\n",
        "\n",
        "dc_hp_preds = dc_random.best_estimator_.predict(X_valid)\n",
        "dc_hp_preds_proba = dc_random.best_estimator_.predict_proba(X_valid)[:,1]\n",
        "roc_auc_score(y_valid, dc_hp_preds_proba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9af302e1",
      "metadata": {
        "id": "9af302e1"
      },
      "outputs": [],
      "source": [
        "recall_score(y_valid, dc_hp_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "588787d3",
      "metadata": {
        "id": "588787d3"
      },
      "source": [
        "## Random Forest - Hyper Parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7f41f64",
      "metadata": {
        "id": "d7f41f64"
      },
      "outputs": [],
      "source": [
        "rf_grid = {'n_estimators':range(200,1000,200), # number of trees\n",
        "           'max_features':['auto','sqrt'], # maximum number of features to use at each split\n",
        "           'max_depth':range(1,11,1), # maximum depth of the tree\n",
        "           'min_samples_split':range(2,10,2), # minimum number of samples to split a node\n",
        "           'criterion':['gini','entropy']} # criterion for evaluating a split\n",
        "\n",
        "rf_random = RandomizedSearchCV(estimator = rf_clf, param_distributions = rf_grid, \n",
        "                               n_iter = 20, cv = 2, scoring=recall_scoring,\n",
        "                               verbose = 1, random_state = 111)\n",
        "\n",
        "rf_random.fit(X_train, y_train)\n",
        "\n",
        "rf_random.best_params_\n",
        "\n",
        "rf_hp_preds = rf_random.best_estimator_.predict(X_valid)\n",
        "rf_hp_preds_proba = rf_random.best_estimator_.predict_proba(X_valid)[:,1]\n",
        "roc_auc_score(y_valid, rf_hp_preds_proba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e9765ce",
      "metadata": {
        "id": "8e9765ce"
      },
      "outputs": [],
      "source": [
        "recall_score(y_valid, rf_hp_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d26f2a0",
      "metadata": {
        "id": "1d26f2a0"
      },
      "source": [
        "## XGBoost - Hyper Parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bd2859b",
      "metadata": {
        "id": "7bd2859b"
      },
      "outputs": [],
      "source": [
        "xgb_grid = params = {\n",
        "        'min_child_weight': [1, 5, 8, 10],\n",
        "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'colsample_bytree': [0.6, 0.8, 0.9, 1.0],\n",
        "        'max_depth': [3, 4, 5]\n",
        "        } # criterion for evaluating a split\n",
        "\n",
        "xgb_random = GridSearchCV(estimator = xgb_clf, param_grid = xgb_grid, \n",
        "                               cv = 2, scoring = recall_scoring,\n",
        "                               verbose = 1)\n",
        "\n",
        "xgb_random.fit(X_train, y_train)\n",
        "\n",
        "xgb_random.best_params_\n",
        "\n",
        "xgb_hp_preds = xgb_random.best_estimator_.predict(X_valid)\n",
        "xgb_hp_preds_proba = xgb_random.best_estimator_.predict_proba(X_valid)[:,1]\n",
        "roc_auc_score(y_valid, xgb_hp_preds_proba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f6d46c6",
      "metadata": {
        "id": "5f6d46c6"
      },
      "outputs": [],
      "source": [
        "recall_score(y_valid, xgb_hp_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f735fa9",
      "metadata": {
        "id": "6f735fa9"
      },
      "source": [
        "### Comparing Hyper Parameter Tuning Results\n",
        "* We can see that even after the hyper-parameter tuning XGB is not performing well as compared to Decision Tree and Random Forest.\n",
        "* To choose the best model we need to take a look at a better AUC and recall score(for our use-case). As per the scores above we can see that the Decision Tree and Random Forest are both close but it would be better to choose Random Forest as it reduces the variance.\n",
        "* We can even try to use Halving Grid Search and try to experiment with more parameters to have an exhaustive search."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f629230",
      "metadata": {
        "id": "3f629230"
      },
      "source": [
        "# Summary of Model Results and Findings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45a8ec85",
      "metadata": {
        "id": "45a8ec85"
      },
      "source": [
        "* We can see that this is a tricky dataset and therefore the normal models might not work, we would need to further use hyper-parameter tuning to improve the performance. To learn about hyper-parameter tuning click [here](https://scikit-learn.org/stable/modules/grid_search.html).\n",
        "* Deep Learning techniques can also be used as they can prove to be really effective in such tricky datasets. One can use `CNN(Convolutional Neural Network)` for this or can also try to use `RNN(Recurrent Neural Network)`.\n",
        "\n",
        "In this notebook we have created a binary classifier to predict the probability that a patient with certain condition would get a stroke or not. On held out test data, our best model had a recall of of 0.83. Using this model, we are able to catch 83% of the patients with stroke correctly. While building the models we have focussed on making sure that we have the least number of false negatives and therefore we have used recall as the metric. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "name": "Copy of HHA550_Stroke-Prediction_Clean-Dataset.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}